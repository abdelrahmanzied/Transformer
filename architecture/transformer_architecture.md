# Transformer Architecture Visualization
This document provides detailed visualizations of the Transformer architecture components and data flow.

## Overall Architecture Flow
```
┌───────────────────────────┐
│       Input Tokens        │
└──────────────┬────────────┘
               ↓
┌───────────────────────────┐
│     Token Embeddings      │
└──────────────┬────────────┘
               ↓
┌───────────────────────────┐
│    Positional Encoding    │
└──────────────┬────────────┘
               ↓
┌───────────────────────────┐
│        Encoder Stack      │ (N identical layers)
└──────────────┬────────────┘
               ↓
┌───────────────────────────┐
│       Encoder Output      │
└──────────────┬────────────┘
               ↓
┌───────────────────────────┐
│        Decoder Stack      │ (N identical layers)
└──────────────┬────────────┘
               ↓
┌───────────────────────────┐
│    Linear + Softmax       │
└──────────────┬────────────┘
               ↓
┌───────────────────────────┐
│       Output Tokens       │
└───────────────────────────┘
```

## Encoder Layer Architecture
```
┌────────────────────────────────────────────────────────┐
│                     Encoder Layer                      │
├────────────────────────────────────────────────────────┤
│                                                        │
│  ┌───────────────────────────────────────────────┐     │
│  │              Input Representations            │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │         Multi-Head Self-Attention             │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │              Add & Normalize                  │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │           Feed-Forward Network                │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │              Add & Normalize                  │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │              Output Representations           │     │
│  └───────────────────────────────────────────────┘     │
└────────────────────────────────────────────────────────┘
```

##  Decoder Layer Architecture
```
┌────────────────────────────────────────────────────────┐
│                     Decoder Layer                      │
├────────────────────────────────────────────────────────┤
│                                                        │
│  ┌───────────────────────────────────────────────┐     │
│  │              Input Representations            │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │       Masked Multi-Head Self-Attention        │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │              Add & Normalize                  │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│  ┌───────────────┐        ↓                            │
│  │  Encoder      │  ┌───────────────────────────┐      │
│  │  Output       │→ │   Cross-Attention         │      │
│  └───────────────┘  └─────┬─────────────────────┘      │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │              Add & Normalize                  │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │           Feed-Forward Network                │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │              Add & Normalize                  │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │              Output Representations           │     │
│  └───────────────────────────────────────────────┘     │
└────────────────────────────────────────────────────────┘
```

## Multi-Head Attention Architecture
```
┌────────────────────────────────────────────────────────┐
│                 Multi-Head Attention                   │
├────────────────────────────────────────────────────────┤
│                                                        │
│  ┌────────────────────────────────────────────────┐    │
│  │                Input Tensor                    │    │
│  └───────────┬────────────┬────────────┬──────────┘    │
│              │            │            │               │
│              ↓            ↓            ↓               │
│  ┌───────────────┐ ┌─────────────┐ ┌─────────────┐     │
│  │ Linear (Q)    │ │ Linear (K)  │ │ Linear (V)  │     │
│  └───────┬───────┘ └──────┬──────┘ └──────┬──────┘     │
│          │                │               │            │
│          ↓                ↓               ↓            │
│  ┌───────────────┐ ┌─────────────┐ ┌─────────────┐     │
│  │ Split Heads   │ │ Split Heads │ │ Split Heads │     │
│  └───────┬───────┘ └──────┬──────┘ └──────┬──────┘     │
│          │                │               │            │
│     ┌────┼────────────────┼───────────────┼────┐       │
│     │    ↓                ↓               ↓    │       │
│     │ ┌──────────────────────────────────────┐ │       │
│     │ │           Attention Head 1           │ │       │
│     │ └────────────────────┬─────────────────┘ │       │
│     │                      │                   │       │
│     │    ↓                 ↓                ↓  │       │
│     │ ┌──────────────────────────────────────┐ │       │
│     │ │           Attention Head 2           │ │       │
│     │ └────────────────────┬─────────────────┘ │       │
│     │                      │                   │       │
│     │            ...       │       ...         │       │
│     │                      │                   │       │
│     │    ↓                 ↓                ↓  │       │
│     │ ┌──────────────────────────────────────┐ │       │
│     │ │           Attention Head h           │ │       │
│     │ └────────────────────┬─────────────────┘ │       │
│     └──────────────────────┼───────────────────┘       │
│                            │                           │
│                            ↓                           │
│  ┌────────────────────────────────────────────────┐    │
│  │                 Concatenate Heads              │    │
│  └────────────────────────┬───────────────────────┘    │
│                           │                            │
│                           ↓                            │
│  ┌────────────────────────────────────────────────┐    │
│  │                 Linear Projection              │    │
│  └────────────────────────┬───────────────────────┘    │
│                           │                            │
│                           ↓                            │
│  ┌────────────────────────────────────────────────┐    │
│  │                 Output Tensor                  │    │
│  └────────────────────────────────────────────────┘    │
└────────────────────────────────────────────────────────┘
```

## Attention Mechanism
```
┌───────────────────────────────────────────────────────────┐
│                   Attention Mechanism                     │
├───────────────────────────────────────────────────────────┤
│                                                           │
│  ┌─────────────────┐  ┌─────────────────┐ ┌─────────────┐ │
│  │      Query      │  │       Key       │ │    Value    │ │
│  └────────┬────────┘  └────────┬────────┘ └──────┬──────┘ │
│           │                    │                 │        │
│           ▼                    ▼                 │        │
│  ┌────────────────────────────────────┐          │        │
│  │        Matrix Multiplication       │          │        │
│  │      (Q × Kᵀ → Attention Scores)   │          │        │
│  └────────────────┬───────────────────┘          │        │
│                   │                              │        │
│                   ▼                              │        │
│  ┌────────────────────────────────────┐          │        │
│  │       Scale by √dₖ (dimension)      │          │        │
│  └────────────────┬───────────────────┘          │        │
│                   │                              │        │
│                   ▼                              │        │
│  ┌────────────────────────────────────┐          │        │
│  │              Softmax               │          │        │
│  │  (Normalize Attention Weights)     │          │        │
│  └────────────────┬───────────────────┘          │        │
│                   │                              │        │
│                   ▼                              ▼        │
│  ┌────────────────────────────────────────────────────┐   │
│  │        Matrix Multiplication with Values           │   │
│  │         (Softmax × V → Weighted Sum)               │   │
│  └────────────────────────┬───────────────────────────┘   │
│                           │                               │
│                           ▼                               │
│  ┌────────────────────────────────────────────────────┐   │
│  │                  Attention Output                  │   │
│  └────────────────────────────────────────────────────┘   │
└───────────────────────────────────────────────────────────┘
```

## Feed-Forward Network
```
┌────────────────────────────────────────────────────────┐
│               Feed-Forward Network                     │
├────────────────────────────────────────────────────────┤
│                                                        │
│  ┌───────────────────────────────────────────────┐     │
│  │              Input (d_model)                  │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │        Linear_1 (d_model → d_ff)              │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │                 ReLU                          │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │               Dropout                         │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │        Linear_2 (d_ff → d_model)              │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │              Output (d_model)                 │     │
│  └───────────────────────────────────────────────┘     │
└────────────────────────────────────────────────────────┘
```

## Layer Normalization
```
┌────────────────────────────────────────────────────────┐
│                 Layer Normalization                    │
├────────────────────────────────────────────────────────┤
│                                                        │
│  ┌───────────────────────────────────────────────┐     │
│  │                 Input Tensor                  │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │     Calculate Mean (across feature dimension) │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │  Calculate Standard Deviation (features dim)  │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │      Normalize: (x - mean) / (std + eps)      │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │        Scale and Shift: α * x_norm + β        │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │               Normalized Output               │     │
│  └───────────────────────────────────────────────┘     │
└────────────────────────────────────────────────────────┘
```

## Positional Encoding
```
┌────────────────────────────────────────────────────────┐
│                 Positional Encoding                    │
├────────────────────────────────────────────────────────┤
│                                                        │
│  ┌───────────────────────────────────────────────┐     │
│  │              Input Embeddings                 │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │   Generate Sinusoidal Position Encodings      │     │
│  │                                               │     │
│  │   For even indices (2i):                      │     │
│  │   PE(pos,2i) = sin(pos/10000^(2i/d_model))    │     │
│  │                                               │     │
│  │   For odd indices (2i+1):                     │     │
│  │   PE(pos,2i+1) = cos(pos/10000^(2i/d_model))  │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │    Add Embeddings + Positional Encodings      │     │
│  └────────────────────────┬──────────────────────┘     │
│                           │                            │
│                           ↓                            │
│  ┌───────────────────────────────────────────────┐     │
│  │     Position-Aware Embeddings Output          │     │
│  └───────────────────────────────────────────────┘     │
└────────────────────────────────────────────────────────┘
```

### Attention Formula Reference
Given matrices:

Q = Queries

K = Keys

V = Values

dₖ = Dimension of key vectors

The attention is computed as:

`Attention(Q, K, V) = softmax((Q × Kᵀ) / √dₖ) × V`